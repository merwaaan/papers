\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[francais]{babel}
\usepackage{amsmath}
\usepackage{float}
\usepackage{tikz}
\usepackage{url}
\usepackage{verbatim}
\usepackage{float}

\usetikzlibrary{arrows, positioning}

\title{Chaînes de Markov}
\author{Merwan Achibet\\Université du Havre}
\date{}

\begin{document}

\maketitle

\section{Concept}

\subsection{Généralités}

Une chaîne de Markov est un processus aléatoire à états pour lequel la
propriété de Markov est vérifiée.

\subsubsection{Processus aléatoire à états}

Un processus aléatoire à états prend la forme d'un vecteur $X$
représentant une liste d'états parcourus, chaque état possible
appartenant à un ensemble d'états $S = \{x_0, \dots, x_n \}$. \`A
intervalle régulier (on parle alors de processus temporellement
homogène), on peut soit rester dans le même état soit passer à un
nouvel état.

Un exemple classique pour illustrer le principe des chaînes de Markov
est celui de la marche aléatoire \cite{ocone} : on veut modéliser
discrètement le déplacement d'un individu dans un environnement à une
dimension, typiquement de gauche à droite. L'ensemble des états peut
ici correspondre à l'indice de la position courante, ainsi $S = \{1,
2, 3, 4, 5\}$ pour 5 positions différentes. Prenons arbitrairement
$X_0 = 2$; l'individu se trouve donc au départ sur la seconde position
à partir de la gauche. \`A chaque étape, l'individu doit se déplacer
vers une position voisine, ce qui revient à lui proposer deux choix
(gauche ou droite), sauf lorsqu'il est à l'une des extrêmités de
l'environnement, cas dans lequel il n'a qu'un seul choix (il va à
droite s'il est à l'extrêmité gauche, et \textit{vice versa}). Lorsque
les deux choix sont possibles, la probabilité que chacun d'entre eux
soit choisi est égale. La figure \ref{marche} illustre cet exemple et
les probabilités de transition entre états voisins.

\begin{figure}[H]
  \centering

  \begin{tikzpicture}[node distance=1cm, auto]

    \tikzstyle{n}=[circle, draw]

    \node[n] (1) {1};
    \node[n, right=of 1] (2) {2};
    \node[n, right=of 2] (3) {3};
    \node[n, right=of 3] (4) {4};
    \node[n, right=of 4] (5) {5};

    \draw[->, thick] (1) to[bend left=60] node[midway] {1} (2);
    \draw[->, thick] (2) to[bend left=60] node[midway] {0.5} (3);
    \draw[->, thick] (3) to[bend left=60] node[midway] {0.5} (4);
    \draw[->, thick] (4) to[bend left=60] node[midway] {0.5} (5);

    \draw[->, thick] (5) to[bend left=60] node[midway] {1} (4);
    \draw[->, thick] (4) to[bend left=60] node[midway] {0.5} (3);
    \draw[->, thick] (3) to[bend left=60] node[midway] {0.5} (2);
    \draw[->, thick] (2) to[bend left=60] node[midway] {0.5} (1);

  \end{tikzpicture}

  \caption{Représentation du problème de la marche aléatoire.}
  \label{marche}

\end{figure}

Usuellement, on se base sur une matrice de transition $T$ pour décrire
les probabilités de passer d'un état précis à un autre. Ici, la
matrice de transition est la suivante :

$$
T = \bordermatrix{
    & 1 & 2 & 3 & 4 & 5 \cr
  1 & 0 & 1 & 0 & 0 & 0 \cr
  2 & 0.5 & 0 & 0.5 & 0 & 0 \cr
  3 & 0 & 0.5 & 0 & 0.5 & 0 \cr
  4 & 0 & 0 & 0.5 & 0 & 0.5 \cr
  5 & 0 & 0 & 0 & 1 & 0 \cr
}
$$
\vspace{0.5cm}

On remarque que tous les $T_{ii}$ avec $i \in \{1, \dots, 5\}$ valent
0, car l'individu ne peut pas rester à la même position pendant deux
étapes successives; la probabilité de rester dans le même état est
donc nulle. De plus, les probabilités de passer à un état représentant
une position non voisine de la position actuelle sont elles aussi
nulles, car l'individu est limité à des déplacements vers les
positions adjacentes. Bien sûr, $\forall i \in \{1, \dots, 5\}$,
$\sum_{j=1}^5 T_{ij} = 1$

\`A partir de $T$, et si l'on veut prévoir la position de l'individu à
la deuxième étape (on cherche donc $X_1$), on remarque que seules les
transitions $2 \rightarrow 1$ et $2 \rightarrow 3$ sont
envisageables. Si l'on dispose d'un générateur de valeurs aléatoires,
on peut alors simuler la décision de l'individu et choisir en
conséquence son état à l'étape suivante. Puisque $P(X_1 = 1 | X_0 = 2)
= 0.5$ et que $P(X_1 = 3 | X_0 = 2) = 0.5$, l'individu a une chance
sur deux d'aller vers la position 1 et une chance sur deux d'aller
vers la position 3.

\subsubsection{Propriété de Markov}

Maintenant que la notion de processus aléatoire à états est présentée,
il faut préciser la propriété de Markov à laquelle une chaîne de
Markov se doit d'obéir.

Concrètement, cette propriété énonce que, dans le cadre d'un processus
aléatoire à états, l'état futur dépend uniquement de l'état présent et
non de tous les états précédents.

Plus formellement, si le processus en question a atteint sa
$t$\textsuperscript{ème} étape :

$$
P(X_{t+1} | X_{t}, X_{t-1}, \dots, X_0) = P(X_{t+1} | X_t)
$$

Autrement dit, seul l'état courant du système importe. Cette propriété
est intéressante, car dans l'exemple de la marche aléatoire, un
processus aléatoire à états basique peut se servir de la matrice de
transition pour déterminer $X_1$ à partir de $X_0$ alors qu'une chaîne
de Markov peut s'en servir pour déterminer $X_{t+1}$ à partir de
n'importe quel $X_t$.

\subsection{Interêts pour la modélisation}

Un système peut être modélisé par un large choix de formalismes. Parmi
les modélisations envisageables, on pense notamment à la modélisation
individu-centrée, ou bien à l'application de lois logistiques ou à
l'utilisation de systèmes d'équations. Un avantage des chaînes de
Markov pour réprésenter un problème est l'éventail de possibilités de
quantification et d'analyse qu'elles offrent \cite{izq}.

Contrairement à une modélisation individu-centrée, de laquelle on ne
peut extraire des analyses qu'à partir des résultats opérationnels,
plusieurs aspects d'un modèle de type chaîne de Markov peuvent être
évalués avant toute éxecution à partir de la matrice de transition. On
peut évidemment prévoir l'état suivant d'une chaîne mais aussi faire
des prévisions sur l'état qui sera atteint au bout de $n$ étapes. Il
est de plus possible d'étudier la périodicité d'une chaîne (si le
modèle admet des cycles dans sa progression) et sa récurrence (le
nombre de fois qu'un état particulier est pris). Nous présentons par
la suite les méthodes de prévision ainsi qu'une classe particulière de
chaînes de Markov, les chaînes absorbantes.

\subsubsection{Prévision}

Comme énoncé précédemment, la propriété de Markov permet de prévoir
l'état suivant d'une chaîne, quel que soit le numéro de l'étape
actuelle. Autrement dit, que l'on en soit à l'étape initiale ou cent
étapes plus tard, on peut évaluer la possibilité de prendre un certain
état à l'étape suivante.

En conséquence, il est aussi possible de calculer la possibilité
d'atteindre un état $j$ à partir d'un état $i$ au bout d'un certain
nombre d'étapes. Reprenons l'exemple de la marche aléatoire, pour
lequel $X_0 = 2$. On veut déterminer la probabilité que l'individu
soit encore à la position 2 à la 3\textsuperscript{ème} étape, soit
deux étapes plus tard. Autrement dit, on veut calculer $P(X_2 = 2 |
X_0 = 2)$. On note cette probabilité $p^{(2)}_{2,2}$ \cite{green}.

On peut représenter conceptuellement toutes les issues possibles de
cette situation par un arbre dans lequel chaque niveau correpond à une
étape (figure \ref{arbre}). On remarque que dans deux cas sur trois,
on atteint l'état 2 à partir de l'état 2.

\begin{figure}[H]
  \centering

  \begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]

    \tikzstyle{n}=[circle, draw]
    \tikzstyle{p}=[rectangle, draw=none, fill=gray!30]

    \node[n] (a) {$2$}
    child {
      node[n] (b) {$1$}
      child {
        node[n] (c) {$2$}
      }
    }
    child {
      node [n] (d) {$3$}
      child {
        node[n] (e) {$2$}
      }
      child {
        node[n] (f) {$4$}
        child[grow=right] {
          node {$t_2$} edge from parent[draw=none]
          child[grow=up] {
            node {$t_1$} edge from parent[draw=none]
            child[grow=up] {
              node {$t_0$} edge from parent[draw=none]
            }
          }
        }
      }
    };

    \draw (a) to node[p, midway] {0.5} (b);
    \draw (b) to node[p, midway] {1} (c);
    \draw (a) to node[p, midway] {0.5} (d);
    \draw (d) to node[p, midway] {0.5} (e);
    \draw (d) to node[p, midway] {0.5} (f);

  \end{tikzpicture}

  \caption{Arbre représentant les états atteignables après deux étapes
    et en partant de la position 2.}
  \label{arbre}

\end{figure}

Pour calculer manuellement $p^{(2)}_{2,2}$, on évalue la probabilité
de chaque branche valide par rapport à toutes les issues possibles :

$$
p^{(2)}_{2,2}
= \frac{p_{2,1} p_{1,2} + p_{2,3} p_{3,2}}{p_{2,1} p_{1,2} + p_{2,3} p_{3,2} + p_{2,3} p_{3,4}}
= \frac{0.5 \times 1 + 0.5 \times 0.5}{0.5 \times 1 + 0.5 \times 0.5 + 0.5 \times 0.5}
= 0.75
$$
\vspace{0.5cm}

Ce calcul, ici simple, se révélera plus fastidieux lorsque la
différence temporelle entre les deux étapes considérées s'agrandira,
car le nombre de feuilles de l'arbre augmentera exponentiellement.

Heureusement, ce même résultat peut être obtenu en exploitant la
matrice de transition. Plus généralement, la probabilité d'être en
l'état $j$ à partir de l'état $i$ après $n$ étape est $p^{(n)}_{ij} =
T^n_{ij}$ \cite{snell}.

$$
T^2 = \bordermatrix{
    & 1 & 2 & 3 & 4 & 5 \cr
  1 & 0.5 & 0 & 0.5 & 0 & 0 \cr
  2 & 0 & 0.75 & 0 & 0.25 & 0 \cr
  3 & 0.25 & 0 & 0.5 & 0 & 0.25 \cr
  4 & 0 & 0.25 & 0 & 0.75 & 0 \cr
  5 & 0 & 0 & 0.5 & 0 & 0.5 \cr
}
$$
\vspace{0.5cm}

Dans ce cas, on retrouve bien $T^2_{2,2} = 0.75$. On note que cette
opération fournit les probabilités de transition projetées vers tous
les états et à partir de tous les états, contrairement au calcul
manuel utilisé plus tôt.

\subsubsection{Absorption}

Parmi les différentes classes de chaînes de Markov, on trouve les
chaînes absorbantes. Cette variante permet des facilités d'analyse si
le modèle associé est adéquat.

Un état absorbant est un état duquel on ne peut pas sortir une fois
atteint. Une chaîne de Markov absorbante est une chaîne de Markov
possédant au moins un état absorbant.

\cite{snell} utilise une variante de la marche aléatoire pour
illustrer le principe des chaînes de Markov absorbantes : l'individu a
un peu trop bu et la position 1 correspond à son domicile tandis que
la position 5 correspond à un bar. S'il atteint un de ces états, il
n'en sort plus, ce sont les états absorbants. Lorsqu'un état absorbant
est atteint, on dit que la chaîne est absorbée.

La nouvelle matrice de transition est :

$$
T = \bordermatrix{
    & 1 & 2 & 3 & 4 & 5 \cr
  1 & 1 & 0 & 0 & 0 & 0 \cr
  2 & 0.5 & 0 & 0.5 & 0 & 0 \cr
  3 & 0 & 0.5 & 0 & 0.5 & 0 \cr
  4 & 0 & 0 & 0.5 & 0 & 0.5 \cr
  5 & 0 & 0 & 0 & 0 & 1 \cr
}
$$
\vspace{0.5cm}

Grâce au principe des chaînes de Markov et à des opérations de base
sur la matrice de transition $T$, si l'on dispose d'une chaîne
absorbante, on peut répondre à des questions telles que :

\begin{enumerate}
  \item{Quelle est la probabilité pour que l'individu finisse par être
    bloqué ?}
  \item{Au bout de combien de déplacements l'individu sera-t-il
    bloqué ?}
  \item{Combien de fois l'individu va-t-il passer par chaque position ?}
\end{enumerate}

Pour répondre à ces questions, il est nécessaire de passer la matrice
de transition sous forme canonique. Ce passage s'opère par permutation
des lignes et des colonnes de la matrice de transition de façon à ce
que les entrées des états absorbants valant 1 forment une matrice
identité de taille $n \times n$ avec $n$ le nombre d'états absorbants.

$$
T = \bordermatrix{
    & 2 & 3 & 4 & 1 & 5 \cr
  2 & \color{red}0 & \color{red}0.5 & \color{red}0 & 0.5 & 0 \cr
  3 & \color{red}0.5 & \color{red}0 & \color{red}0.5 & 0 & 0 \cr
  4 & \color{red}0 & \color{red}0.5 & \color{red}0 & 0 & 0.5 \cr
  1 & 0 & 0 & 0 & \color{blue}1 & \color{blue}0 \cr
  5 & 0 & 0 & 0 & \color{blue}0 & \color{blue}1 \cr
}
=
\begin{pmatrix}
  \color{red}Q & * \\
  0 & \color{blue}I
\end{pmatrix}
$$
\vspace{0.5cm}

On décompose donc $T$ en :

$$
Q =
\begin{pmatrix}
  0 & 0.5 & 0 \\
  0.5 & 0 & 0.5 \\
  0 & 0.5 & 0
\end{pmatrix}
\;\;\;\;\;
I =
\begin{pmatrix}
  1 & 0 \\
  0 & 1
\end{pmatrix}
$$
\vspace{0.5cm}

Si l'on souhaite par exemple estimer le nombre moyen de passages par
chaque état avant l'absorption de la chaîne, on calcule la matrice
fondamentale $N$ telle que :

$$
N = (I - Q)^{-1}
$$

Chaque ligne de $N$ correspond au nombre de passage attendu par chaque
autre état en ayant pris l'état de la ligne comme point de
départ. Dans le cas de la marche aléatoire alcoolisée, on prévoit que
l'individu commençant son trajet à la position 3 passera en moyenne
une fois sur la position 2 avant absorption.

$$
N =
\bordermatrix{
  & 2 & 3 & 4 \cr
  2 & 1.5 & 1 & 0.5 \cr
  3 & 1 & 2 & 1 \cr
  4 & 0.5 & 1 & 1.5
}
$$

\section{Application : D'Artémis à Zeus}

Les chaînes de Markov sont couramment utilisées pour générer du texte
de façon aléatoire \cite{shan}, ou même de la musique \cite{conk}. En
considérant qu'un état est un symbole (lettre, syllabe, mot ou phrase)
et que la succession d'états parcourus par la chaîne forme une
composition de symboles, on peut obtenir des textes donnant une
impression d'authenticité. Pour obtenir un résultat satisfaisant, il
sera bien sûr nécessaire d'utiliser une matrice de transition
adaptée. Typiquement, elle sera calculée à partir de l'étude
statistique d'un corpus contenant les textes à imiter.

Nous allons implémenter un générateur de noms inspiré de ceux des
protagonistes de la mythologie grecque. Il prendra en entrée un corpus
de noms grecs et générera un nouveau nom adoptant les mêmes
caractéristique structurelles.

\subsection{Modèle}

Dans le cadre de cet exemple, la chaîne de Markov correspond à un nom,
soit un mot. Un mot est composé de lettres, chacune correspondant à un
état. Ici, l'ensemble des états est donc $S = \{a, b, c, \dots, z, \xi
\}$, avec $\xi$ le symbole de fin de mot.

L'état initial $X_0$ sera tiré au hasard parmi cet alphabet, en
ignorant $\xi$. On passe ensuite d'état en état pour construire
progressivement le mot, jusqu'à ce que l'on atteigne $\xi$. Cet état
fait ici office d'état absorbant puisqu'on ne peut plus en sortir une
fois qu'il est atteint. Ce symbole de fin de mot est nécessaire à la
génération de noms convaincants puisque que c'est le passage vers
l'état le représentant qui déterminera l'arrêt de la construction et
donc la longueur du mot.

L'opération se déroulera en deux phases principales. Tout d'abord, la
matrice de transition $T$, qui nous permet de déterminer quel parcours
suivre, doit être calculée. Pour cela, on se basera sur un corpus de
noms de divinités grecques que nous allons parcourir afin de relever
les relations de succession entre les différentes lettres de notre
alphabet. Une fois $T$ obtenue, il reste à construire la chaîne de
Markov de façon classique. La succession d'états parcourus formera le
nouveau nom.

\subsection{Implémentation}

Cette implémentation a été réalisée en Javascript\footnote{Sources
  consultables sur
  \url{http://github.com/merwaaan/papers/blob/master/markov/markov.js}}. Le
corpus utilisé est une sélection de noms issus de
\url{http://en.wikipedia.org/wiki/List_of_Greek_mythological_figures}

Le corpus, contenant 45 noms grecs, est fourni à notre programme. Le
processus d'apprentissage comptabilise les relations de succession
entre chaque symbole et permet par la suite la génération de noms
structurellement équivalents. Plus la taille du corpus est élevée et
plus les noms produits seront convaincants car le poids des relations
entre les symboles formant des syllabes caractéristiques en seront
renforcés.

\verbatiminput{corpus.txt}

On note que le symbole de fin de mot n'est pas explicitement précisé
dans le corpus mais \textit{demether} est bien égal à
\textit{demether}$\xi$.

On commence par parcourir chacune des entrées de ce dictionnaire pour
compter les relations de succession entre lettres voisines. On dispose
au départ d'une matrice de dimensions $27 \times 27$ (26 lettres et le
symbole de fin de mot) dont toutes les cases sont nulles. \`A chaque
succession de la lettre $i$ par la lettre $j$, on incrémente
$T_{ij}$. On obtient à la fin de ce traitement une matrice équivalente
à :

$$
T = \bordermatrix{
    & a & b & c & \dots \cr
  a & 0 & 1 & 3 & \dots \cr
  b & 1 & 0 & 0 & \dots \cr
  c & 4 & 0 & 0 & \dots \cr
  \vdots & \vdots & \vdots & \vdots & \ddots \cr
}
$$
\vspace{0.5cm}

Il est ensuite nécessaire d'en normaliser les valeurs afin que la
somme des éléments de chaque ligne donne 1. On note cette matrice
normalisée $T'$ et on la calcule telle que :

$$
\forall i, j \in S,\;\;\;\;\; {T'}_{ij} = \frac{T_{ij}}{\sum_{k \in S} T_{ik}}
$$
\vspace{0.5cm}

On obtient au final une matrice $T'$ équivalente à :

$$
T' = \bordermatrix{
    & a & b & c & \dots \cr
  a & 0 & 0.0153 & 0.054 & \dots \cr
  b & 0.12 & 0 & 0 & \dots \cr
  c & 0.32 & 0 & 0 & \dots \cr
  \vdots & \vdots & \vdots & \vdots & \ddots \cr
}
$$
\vspace{0.5cm}

Une fois la matrice de transition calculée, la génération se fait de
façon classique. Un état initial est choisi aléatoirement dans $S$ et
on passe d'état en état jusqu'à atteindre l'état absorbant $\xi$. La
sélection de chaque lettre est effectuée par une fonction utilisant un
algorithme de roue de la fortune.

\subsection{Résultats}

La table \ref{bons} contient une sélection de noms générés par notre
programme utilisant les chaînes de Markov. On note que les majuscules
ont été ajoutées en fin de traitement à des fins de présentation car
nos états représentent tous des lettres en minuscule.

Les résultats sont satisfaisants et on s'imagine facilement les
épopées de Tallonyx, Erapithes et Mitheron.

\begin{table}[H]
  \centering

  \begin{tabular}{l|l|l}
    Diaetheron & Therisethe & Temes \\
    Omeneus & Iophemion & Usthypios \\
    Crymiarene & Phyostalal & Erapithes \\
    Arito & Zethetus & Anurysthes \\
    Nepadion & Tallonyx & Bolemeprio \\
    Hopheleus & Onthonophe & Phaleria \\
    Bonus & Stionys & Mitheron
  \end{tabular}

  \caption{Liste de noms générés et convaincants.}
  \label{bons}

\end{table}

Cependant d'autres résultats semblent moins réalistes et on se rend
compte des faiblesses des chaînes de markov en observant le tableau
\ref{pasbons}. On y retrouve des noms trop courts, trop longs,
répétitifs ou bien difficilement prononçables.

\begin{table}[H]
  \centering

  \begin{tabular}{l|l|l}
    R & La & Chrthellyx \\
    Pen & Memememese & Bonysteidianataeronetisus \\
    Ymiodiollclllchete & Crme & X
  \end{tabular}

  \caption{Liste de noms générés moins convaincants.}
  \label{pasbons}

\end{table}

Les longueurs innapropriées de certains noms sont inévitables à moins
d'ajouter des limites minimale et maximale dures à notre programme. Ce
problème est en fait lié à la propriété de Markov : puisqu'une chaîne
ne se soucie que de l'état présent, elle n'a aucune connaissance des
états précédents. En conséquence, nous ne disposons pas d'une vue
d'ensemble du nom pendant sa construction et il peut finir soit très
tôt soit très tard.

Les noms composées d'une unique lettre peuvent découler du problème
décrit précedemment mais aussi d'un corpus trop réduit. On obtient par
exemple le nom X à cause du fait que le symbole $x$ est uniquement
présent dans l'entrée \textit{styx} du corpus. Ainsi ${T'}_{x\xi} = 1$
et si le premier état choisi aléatoirement est $x$, le mot est déjà
terminé ! pour réduire les occurrences de ce problème, il faudra
agrandir le corpus afin d'éviter les caractères disposant de peu
de successeurs.

Autre soucis, certains noms sont imprononçables. En effet, la
propriété de Markov pose encore une limite puisque la chaîne ne
considère que les lettres voisines et non les syllabes dans leur
ensemble. Pour résoudre ce problème, on pourrait imaginer utiliser des
syllabes comme état plutôt que des lettres. L'isolation de syllabes
est une tâche complexe mais on pourrait, plus généralement, utiliser
des blocs de lettres de longueur $n$ comme état. De cette façon, on
réduirait le nombre de syllabes incohérentes. Un corpus beaucoup plus
important serait néanmoins nécessaire pour contrebalancer le fait que
le nombre d'états augmenterait considérablement.

\bibliographystyle{alpha}
\bibliography{sources}

\end{document}
